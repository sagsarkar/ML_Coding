{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa+2HnfGhDdnfzw5jPFdNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagsarkar/ML_Coding/blob/main/deep_learning_layers_in_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2HfNhDcNhsZd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout layer"
      ],
      "metadata": {
        "id": "uRhrmIsph5MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutLayer:\n",
        "  def __init__(self, dropout_rate):\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x, is_training=True):\n",
        "    if is_training:\n",
        "      self.mask = (np.random.rand(*x.shape) < (1 - self.dropout_rate)) / (1-self.dropout_rate)\n",
        "      output = self.mask * x\n",
        "    else:\n",
        "      output = x\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backward(self, output_gradient):\n",
        "    if self.mask is not None:\n",
        "      input_gradient = self.mask * output_gradient\n",
        "      return input_gradient\n",
        "    else:\n",
        "      return output_gradient\n",
        "\n",
        "# Test the DropoutLayer\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(5, 5)\n",
        "dropout_layer = DropoutLayer(dropout_rate=0.3)\n",
        "\n",
        "print(\"Input:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nDuring Training:\")\n",
        "output_train = dropout_layer.forward(x, is_training=True)\n",
        "print(output_train)\n",
        "\n",
        "# Backpropagation example (assuming some gradient from subsequent layers)\n",
        "grad_output = np.random.rand(*x.shape)\n",
        "print(\"\\nGradient w.r.t. output:\\n\", grad_output)\n",
        "\n",
        "grad_input = dropout_layer.backward(grad_output)\n",
        "print(\"\\nGradient w.r.t. input:\\n\", grad_input)\n",
        "\n",
        "print(\"\\nDuring Inference:\")\n",
        "output_inference = dropout_layer.forward(x, is_training=False)\n",
        "print(output_inference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZaAHd4Hnugj",
        "outputId": "22aef1b8-4327-493b-e3a5-514e2008fdac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "[[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
            " [0.15599452 0.05808361 0.86617615 0.60111501 0.70807258]\n",
            " [0.02058449 0.96990985 0.83244264 0.21233911 0.18182497]\n",
            " [0.18340451 0.30424224 0.52475643 0.43194502 0.29122914]\n",
            " [0.61185289 0.13949386 0.29214465 0.36636184 0.45606998]]\n",
            "\n",
            "During Training:\n",
            "[[0.         1.35816329 1.04570563 0.85522641 0.22288377]\n",
            " [0.22284931 0.08297659 1.23739449 0.         0.        ]\n",
            " [0.         1.3855855  1.18920377 0.30334159 0.25974995]\n",
            " [0.26200644 0.43463178 0.74965205 0.         0.41604163]\n",
            " [0.87407556 0.19927694 0.4173495  0.52337406 0.65152855]]\n",
            "\n",
            "Gradient w.r.t. output:\n",
            " [[0.96958463 0.77513282 0.93949894 0.89482735 0.59789998]\n",
            " [0.92187424 0.0884925  0.19598286 0.04522729 0.32533033]\n",
            " [0.38867729 0.27134903 0.82873751 0.35675333 0.28093451]\n",
            " [0.54269608 0.14092422 0.80219698 0.07455064 0.98688694]\n",
            " [0.77224477 0.19871568 0.00552212 0.81546143 0.70685734]]\n",
            "\n",
            "Gradient w.r.t. input:\n",
            " [[0.         1.1073326  1.34214135 1.27832479 0.85414283]\n",
            " [1.31696319 0.12641786 0.27997552 0.         0.        ]\n",
            " [0.         0.38764147 1.18391073 0.50964761 0.40133501]\n",
            " [0.77528012 0.20132032 1.14599569 0.         1.40983848]\n",
            " [1.10320681 0.28387955 0.00788874 1.1649449  1.00979621]]\n",
            "\n",
            "During Inference:\n",
            "[[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
            " [0.15599452 0.05808361 0.86617615 0.60111501 0.70807258]\n",
            " [0.02058449 0.96990985 0.83244264 0.21233911 0.18182497]\n",
            " [0.18340451 0.30424224 0.52475643 0.43194502 0.29122914]\n",
            " [0.61185289 0.13949386 0.29214465 0.36636184 0.45606998]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention"
      ],
      "metadata": {
        "id": "CaSGo9XocH0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer:\n",
        "    def __init__(self, hid_dim, n_heads, dropout):\n",
        "        assert hid_dim % n_heads == 0, \"hidden_dim should be divisible by num_heads\"\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "\n",
        "        # Fully connected layers for query, key, value, and output\n",
        "        self.fc_q = lambda x: np.dot(x, np.random.randn(self.hid_dim, self.hid_dim))\n",
        "        self.fc_k = lambda x: np.dot(x, np.random.randn(self.hid_dim, self.hid_dim))\n",
        "        self.fc_v = lambda x: np.dot(x, np.random.randn(self.hid_dim, self.hid_dim))\n",
        "        self.fc_o = lambda x: np.dot(x, np.random.randn(self.hid_dim, self.hid_dim))\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = lambda x: x * np.random.choice([0, 1], size=x.shape, p=[dropout, 1 - dropout])\n",
        "\n",
        "        self.scale = np.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, query, key, value, src_mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Linear transformations for query, key, and value\n",
        "        Q = self.fc_q(query)  # [batch_size, query_len, hid_dim]\n",
        "        K = self.fc_k(key)    # [batch_size, key_len, hid_dim]\n",
        "        V = self.fc_v(value)  # [batch_size, value_len, hid_dim]\n",
        "\n",
        "        # Reshape and transpose for multi-head attention\n",
        "        Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).transpose(0, 2, 1, 3)  # [batch_size, n_heads, query_len, head_dim]\n",
        "        K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).transpose(0, 2, 1, 3)  # [batch_size, n_heads, key_len, head_dim]\n",
        "        V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).transpose(0, 2, 1, 3)  # [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # Compute scaled dot-product attention energy\n",
        "        energy = np.matmul(Q, K.transpose(0, 1, 3, 2)) / self.scale  # [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # Apply source sequence mask if provided\n",
        "        if src_mask is not None:\n",
        "            energy = np.where(src_mask == 0, -1e10, energy)\n",
        "\n",
        "        # Apply softmax to get attention scores\n",
        "        attention = np.exp(energy - np.max(energy, axis=-1, keepdims=True))  # [batch_size, n_heads, query_len, key_len]\n",
        "        attention /= np.sum(attention, axis=-1, keepdims=True)  # Normalize attention scores\n",
        "\n",
        "        # Apply dropout to attention scores and compute weighted sum with values\n",
        "        x = np.matmul(self.dropout(attention), V)  # [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        # Transpose and reshape back to original shape\n",
        "        x = x.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.hid_dim)  # [batch_size, query_len, hid_dim]\n",
        "\n",
        "        # Linear transformation for the final output\n",
        "        x = self.fc_o(x)  # [batch_size, query_len, hid_dim]\n",
        "\n",
        "        return x, attention\n",
        "\n",
        "# Test the MultiHeadAttentionLayer\n",
        "hid_dim = 64\n",
        "n_heads = 8\n",
        "dropout = 0.1\n",
        "\n",
        "batch_size = 4\n",
        "sentence_len = 10\n",
        "\n",
        "sentence = np.random.randn(batch_size, sentence_len, hid_dim)\n",
        "src_mask = np.random.choice([0, 1], size=(batch_size, 1, 1, sentence_len), p=[0.1, 0.9])  # Example source mask\n",
        "\n",
        "attention_layer = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
        "output, attention = attention_layer.forward(sentence, sentence, sentence, src_mask)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention shape:\", attention.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W44KlLPw1nAu",
        "outputId": "2014d0e5-3f19-40a5-a68a-0a5384504190"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (4, 10, 64)\n",
            "Attention shape: (4, 8, 10, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense Layer with skip connection"
      ],
      "metadata": {
        "id": "b9wyVCoT9F1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayerWithSkip:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        self.z = np.dot(x, self.weights) + self.bias\n",
        "        return self.z + x\n",
        "\n",
        "    def backward(self, dz):\n",
        "        self.db = np.sum(dz, axis=0)\n",
        "        self.dW = np.dot(self.x.T, dz)\n",
        "        dx_skip = dz\n",
        "        dx_dense = np.dot(dz, self.weights.T)\n",
        "\n",
        "        dx_input = dx_skip + dx_dense  # Overall input gradient\n",
        "\n",
        "        return dx_input\n",
        "\n",
        "# Testing the class\n",
        "input_size = 5\n",
        "output_size = 5\n",
        "\n",
        "layer = DenseLayerWithSkip(input_size, output_size)\n",
        "\n",
        "# Forward pass\n",
        "x = np.random.randn(10, input_size)\n",
        "output = layer.forward(x)\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "# Backward pass\n",
        "dz = np.random.randn(10, output_size)\n",
        "dx_input = layer.backward(dz)\n",
        "print(\"Overall input gradient shape:\", dx_input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pXldwWD9FGm",
        "outputId": "4bca1cb1-1951-4bcb-f177-577031210bb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (10, 5)\n",
            "Overall input gradient shape: (10, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer normalization"
      ],
      "metadata": {
        "id": "UqWhJG1R_tQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization:\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = np.ones(1)  # Initialize gamma to ones\n",
        "        self.beta = np.zeros(1)  # Initialize beta to zeros\n",
        "        self.mean = None\n",
        "        self.variance = None\n",
        "        self.normalized_input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        self.variance = np.var(x, axis=-1, keepdims=True)\n",
        "        self.normalized_input = (x - self.mean) / np.sqrt(self.variance + self.epsilon)\n",
        "\n",
        "        y = self.gamma * self.normalized_input + self.beta\n",
        "        return y\n",
        "\n",
        "# Example usage\n",
        "batch_size = 32\n",
        "sentence_length = 10\n",
        "hidden_dimension = 64\n",
        "\n",
        "# Generate random input tensor\n",
        "input_tensor = np.random.randn(batch_size, sentence_length, hidden_dimension)\n",
        "\n",
        "# Create a LayerNormalization instance\n",
        "layer_norm = LayerNormalization()\n",
        "\n",
        "# Forward pass\n",
        "output_tensor = layer_norm.forward(input_tensor)\n",
        "\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "print(\"Output shape:\", output_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49R7a3tB9M-M",
        "outputId": "e5db09c6-e33d-4031-834c-28e2d92965cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (32, 10, 64)\n",
            "Output shape: (32, 10, 64)\n"
          ]
        }
      ]
    }
  ]
}